{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"NextCV: Fast Computer Vision in Python C++ performance, Python simplicity. The best of both worlds. <p>Our Philosophy: Pragmatic Performance</p> <p>We believe in a simple but powerful idea: use Python for everything, until it's not fast enough. Then, and only then, do we bring in C++ to optimize the bottlenecks.</p> <ul> <li>\ud83d\udc0d Python First: Rapid prototyping and development.</li> <li>\ud83d\ude80 C++ for Speed: When performance is critical.</li> <li>\ud83c\udfaf Strategic Optimization: We don't rewrite everything, just what matters.</li> </ul>"},{"location":"#-get-started-in-2-minutes","title":"\ud83d\ude80 Get Started in 2 Minutes","text":"<p>Ready to give it a try? Our Getting Started guide will have you up and running in just a few minutes.</p>"},{"location":"#-learn-more","title":"\ud83e\udd14 Learn More","text":"<ul> <li>Why NextCV? Learn about our approach to \"smart performance\".</li> <li>When to Use C++? Our guide to making pragmatic performance decisions.</li> <li>API Reference A full list of available functions.</li> </ul>"},{"location":"#-buy-me-a-coffee","title":"\u2615\ufe0f Buy Me a Coffee","text":"<p>Primpting AI agents is hard! Coffees are needed when AI agents derail so much I actually have to write code.</p> <p></p>"},{"location":"contributing/","title":"Contributing to NextCV","text":"<p>First off, thank you for considering contributing to NextCV! We welcome all contributions, from bug fixes to new features and documentation improvements.</p>"},{"location":"contributing/#-how-to-contribute","title":"\ud83d\ude80 How to Contribute","text":"<p>Quick Setup</p> <p>We use uv as our package manager - it's like pip, but much faster! Learn more in the uv documentation.</p> <p>We recommend following this workflow to contribute:</p> <ol> <li>Fork the repository: Create your own copy of the project.</li> <li>Create a feature branch: <code>git checkout -b my-new-feature</code></li> <li> <p>Install Python tools:</p> <pre><code># Install uv (if you don't have it)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> </li> <li> <p>Install C++ tools:</p> macOSUbuntu/Debian <pre><code>brew install clang-format llvm\n# Add LLVM to PATH (add to ~/.zshrc or ~/.bash_profile)\necho 'export PATH=\"/usr/local/opt/llvm/bin:$PATH\"' &gt;&gt; ~/.zshrc\n</code></pre> <pre><code>sudo apt-get install clang-format clang-tidy\n</code></pre> </li> <li> <p>Set up the development environment:</p> <pre><code># Install dependencies with uv (blazing fast Python package manager)\nuv sync\n\n# Install pre-commit hooks\nuvx pre-commit install\n</code></pre> </li> <li> <p>Make your changes: Write your code and add tests for it.</p> </li> <li> <p>Run tests and code quality checks:</p> <pre><code># Run tests\nuv run pytest\n\n# Run pre-commit checks\nuvx pre-commit run --all-files\n</code></pre> </li> <li> <p>Commit your changes: <code>git commit -m 'Add some feature'</code></p> </li> <li>Push to your branch: <code>git push origin my-new-feature</code></li> <li>Submit a pull request: Open a pull request from your fork to the main NextCV repository.</li> </ol>"},{"location":"contributing/#-what-to-contribute","title":"\ud83d\udca1 What to Contribute","text":"<p>Contribution Ideas</p> <p>Not sure where to start? Here are a few ideas:</p> <pre><code>- **\ud83d\udc1b Bug fixes:** Look for open issues with the `bug` label.\n- **\u2728 New features:** Propose a new feature by opening an issue to discuss it first.\n- **\ud83d\udcda Documentation:** Improve the documentation, add examples, or write tutorials.\n- **\u26a1\ufe0f Performance improvements:** Find bottlenecks and optimize them.\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will walk you through installing NextCV and running your first performance demo.</p>"},{"location":"getting-started/#1-prerequisites","title":"1. Prerequisites","text":"LinuxmacOS <pre><code>sudo apt-get install libeigen3-dev cmake\n</code></pre> <pre><code>brew install eigen cmake\n</code></pre>"},{"location":"getting-started/#2-installation","title":"2. Installation","text":"pipuv <pre><code>pip install git+https://github.com/kevinconka/nextcv.git\n</code></pre> <pre><code>uv add git+https://github.com/kevinconka/nextcv.git\n</code></pre>"},{"location":"getting-started/#3-performance-demo","title":"3. Performance Demo","text":"<p>This example demonstrates the performance difference between the C++ and NumPy implementations of Non-Maximum Suppression (NMS).</p> <pre><code>import time\nimport numpy as np\nfrom nextcv.postprocessing import nms_cpp, nms_np\n\n# Create a large dataset of bounding boxes\nN = 10000\nrng = np.random.default_rng(42)\nbboxes = rng.uniform(0, 100, (N, 4)).astype(np.float32)\nscores = rng.uniform(0.1, 1, N).astype(np.float32)\n\n# Time C++ implementation\nstart_time = time.perf_counter()\nresult_cpp = nms_cpp(bboxes, scores, 0.5)\ncpp_time = time.perf_counter() - start_time\n\n# Time NumPy implementation\nstart_time = time.perf_counter()\nresult_np = nms_np(bboxes, scores, 0.5)\nnp_time = time.perf_counter() - start_time\n\nprint(\"Post-processing (NMS timing comparison):\")\nprint(f\"   Dataset: {len(bboxes)} bounding boxes\")\nprint(f\"   nms_cpp(): {len(result_cpp)} boxes kept in {cpp_time * 1000:.2f}ms\")\nprint(f\"   nms_np(): {len(result_np)} boxes kept in {np_time * 1000:.2f}ms\")\n</code></pre> <p>Output:</p> <pre><code>Post-processing (NMS timing comparison):\n   Dataset: 10000 bounding boxes\n   nms_cpp(): 950 boxes kept in 25.93ms\n   nms_np(): 949 boxes kept in 65.39ms\n</code></pre>"},{"location":"getting-started/#4-next-steps","title":"4. Next Steps","text":"<p>Now that you have NextCV installed, you can start exploring the API.</p> <ul> <li>Explore the API: Check out the API Reference for a full list of available functions.</li> <li>Learn about our philosophy: Read our guide on When to Use C++ to understand our approach to performance.</li> </ul>"},{"location":"examples/","title":"NextCV Examples","text":"<p>Quick examples showing how to use NextCV for computer vision tasks.</p>"},{"location":"examples/#-core-module","title":"\ud83d\udce6 Core Module","text":"<pre><code>import nextcv as cvx\n\n# Hello functions\nprint(cvx.core.hello_cpp())     # \"Hello from C++!\"\nprint(cvx.core.hello_python())  # \"Hello from Python!\"\n</code></pre>"},{"location":"examples/#-image-module","title":"\ud83d\uddbc\ufe0f Image Module","text":"<pre><code>import numpy as np\nimport nextcv as cvx\n\n# Image inversion\nimage = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\ninverted = cvx.image.invert(image)\nprint(f\"Shape: {image.shape} -&gt; {inverted.shape}\")\n</code></pre>"},{"location":"examples/#-postprocessing-module","title":"\ud83c\udfaf Postprocessing Module","text":"<pre><code>import numpy as np\nimport nextcv as cvx\n\n# Non-Maximum Suppression\nboxes = np.array([[10, 10, 50, 50], [20, 20, 60, 60]], dtype=np.float32)\nscores = np.array([0.9, 0.8], dtype=np.float32)\n\nfiltered = cvx.postprocessing.nms(boxes, scores, threshold=0.5)\nprint(f\"Kept {len(filtered)} out of {len(boxes)} boxes\")\n</code></pre>"},{"location":"examples/#-linear-algebra-module","title":"\ud83d\udd22 Linear Algebra Module","text":"<pre><code>import numpy as np\nimport nextcv as cvx\n\n# Matrix-vector multiplication using Eigen\nA = np.random.randn(4, 3).astype(np.float32)\nx = np.random.randn(3).astype(np.float32)\n\ny = cvx.linalg.matvec(A, x)\nprint(f\"Result shape: {y.shape}, dtype: {y.dtype}\")\n</code></pre>   **Ready to build something amazing?** \ud83d\ude80  [Get Started](getting-started.md) \u2022 [View API](reference/) \u2022 [Contribute](pybind11-guide.md)"},{"location":"pybind11/","title":"Pybind11: Python \ud83e\udd1d C++","text":"<p>Welcome to the NextCV development guide. Here, we'll show you how to make your Python code blazing fast when it matters.</p>"},{"location":"pybind11/#-the-philosophy-smart-performance","title":"\ud83c\udfaf The Philosophy: Smart Performance","text":"<p>Our Approach</p> <p>We don't rewrite everything in C++. That would be like using a Formula 1 car to go to the grocery store. Instead, we're pragmatic.</p> <p>We follow a simple, three-step process:</p> <ol> <li>Prototype in Python: Build and test your ideas quickly.</li> <li>Identify Bottlenecks: Find the parts of your code that are actually slow.</li> <li>Optimize with C++: Rewrite only the critical parts for maximum impact.</li> </ol> <p>This gives us the best of both worlds:</p> Feature Python  C++  Speed Fast Prototyping Raw Performance Usage General Purpose Critical Bottlenecks Ecosystem Rich &amp; Extensive Focused &amp; Performant <p>Key Insight</p> <p>We're not performance-obsessed, we're value-obsessed. We only add C++ when it provides a real, measurable benefit.</p>"},{"location":"pybind11/best-practices/","title":"Best Practices","text":"<p>Building maintainable Python-C++ bindings requires following established patterns. Here are the rules that actually matter.</p>"},{"location":"pybind11/best-practices/#naming-conventions","title":"Naming Conventions","text":""},{"location":"pybind11/best-practices/#c-functions","title":"C++ Functions","text":"<pre><code>// Good: snake_case for functions\nauto conv2d(const std::vector&lt;float&gt;&amp; input) -&gt; std::vector&lt;float&gt;;\nauto gaussian_blur(const Image&amp; image, float sigma) -&gt; Image;\n\n// Good: PascalCase for classes\nclass ImageProcessor {\npublic:\n    void process_image(const Image&amp; input);\n};\n</code></pre>"},{"location":"pybind11/best-practices/#python-functions","title":"Python Functions","text":"<pre><code># Good: snake_case with suffixes\ndef conv2d_cpp(image, kernel):\n    \"\"\"C++ implementation.\"\"\"\n    pass\n\ndef conv2d_python(image, kernel):\n    \"\"\"Python implementation.\"\"\"\n    pass\n\ndef conv2d(image, kernel):\n    \"\"\"Smart wrapper that chooses implementation.\"\"\"\n    pass\n</code></pre>"},{"location":"pybind11/best-practices/#code-organization","title":"Code Organization","text":""},{"location":"pybind11/best-practices/#file-structure","title":"File Structure","text":"<pre><code>nextcv/\n\u251c\u2500\u2500 _cpp/src/\n\u2502   \u251c\u2500\u2500 core/           # Core functionality\n\u2502   \u251c\u2500\u2500 image/          # Image processing\n\u2502   \u251c\u2500\u2500 tracking/       # Tracking algorithms\n\u2502   \u2514\u2500\u2500 bindings/       # PyBind11 bindings\n\u251c\u2500\u2500 core/               # Python core modules\n\u251c\u2500\u2500 image/              # Python image modules\n\u2514\u2500\u2500 tracking/           # Python tracking modules\n</code></pre>"},{"location":"pybind11/best-practices/#binding-organization","title":"Binding Organization","text":"<pre><code>// Group related functions\nmodule.def(\"conv2d\", &amp;conv2d, \"Apply 2D convolution\");\nmodule.def(\"gaussian_blur\", &amp;gaussian_blur, \"Apply Gaussian blur\");\nmodule.def(\"edge_detection\", &amp;edge_detection, \"Detect edges\");\n</code></pre>"},{"location":"pybind11/best-practices/#error-handling","title":"Error Handling","text":""},{"location":"pybind11/best-practices/#input-validation","title":"Input Validation","text":"<pre><code>py::array_t&lt;float&gt; my_function(py::array_t&lt;float&gt; input) {\n    // Always validate inputs\n    if (input.ndim() != 2) {\n        throw std::runtime_error(\"Input must be 2D\");\n    }\n\n    if (!(input.flags() &amp; py::array::c_style)) {\n        throw std::runtime_error(\"Input must be C-contiguous\");\n    }\n\n    // Process data...\n}\n</code></pre>"},{"location":"pybind11/best-practices/#exception-safety","title":"Exception Safety","text":"<pre><code>// Use RAII for automatic cleanup\nclass ImageProcessor {\nprivate:\n    std::vector&lt;float&gt; data_;\n\npublic:\n    ImageProcessor(size_t size) : data_(size) {}\n    // Destructor automatically cleans up\n};\n</code></pre>"},{"location":"pybind11/best-practices/#memory-management","title":"Memory Management","text":""},{"location":"pybind11/best-practices/#efficient-array-handling","title":"Efficient Array Handling","text":"<pre><code>py::array_t&lt;float&gt; process_array(py::array_t&lt;float&gt; input) {\n    // Get buffer info once\n    py::buffer_info buf = input.request();\n    float* ptr = static_cast&lt;float*&gt;(buf.ptr);\n\n    // Process in-place when possible\n    for (size_t i = 0; i &lt; buf.size; ++i) {\n        ptr[i] = process_pixel(ptr[i]);\n    }\n\n    return input; // Return same array\n}\n</code></pre>"},{"location":"pybind11/best-practices/#avoid-unnecessary-copies","title":"Avoid Unnecessary Copies","text":"<pre><code>// Good: Pass by reference\nvoid process_image(const std::vector&lt;float&gt;&amp; input, std::vector&lt;float&gt;&amp; output);\n\n// Bad: Pass by value\nvoid process_image(std::vector&lt;float&gt; input, std::vector&lt;float&gt; output);\n</code></pre>"},{"location":"pybind11/best-practices/#performance-tips","title":"Performance Tips","text":""},{"location":"pybind11/best-practices/#use-c-contiguous-arrays","title":"Use C-Contiguous Arrays","text":"<pre><code>// Always check for C-contiguity\nif (!(input.flags() &amp; py::array::c_style)) {\n    throw std::runtime_error(\"Input must be C-contiguous\");\n}\n</code></pre>"},{"location":"pybind11/best-practices/#optimize-hot-paths","title":"Optimize Hot Paths","text":"<pre><code>// Use const references for large objects\nvoid process_large_data(const std::vector&lt;float&gt;&amp; data) {\n    // Process data...\n}\n</code></pre>"},{"location":"pybind11/best-practices/#profile-before-optimizing","title":"Profile Before Optimizing","text":"<pre><code>import cProfile\nimport nextcv as cvx\n\n# Profile your function\ncProfile.run('cvx.tracking.hungarian_algorithm(cost_matrix)')\n</code></pre>"},{"location":"pybind11/best-practices/#documentation","title":"Documentation","text":""},{"location":"pybind11/best-practices/#function-documentation","title":"Function Documentation","text":"<pre><code>/**\n * @brief Apply 2D convolution to an image\n * @param input Input image array\n * @param kernel Convolution kernel\n * @return Convolved image array\n */\npy::array_t&lt;float&gt; conv2d(py::array_t&lt;float&gt; input, py::array_t&lt;float&gt; kernel);\n</code></pre>"},{"location":"pybind11/best-practices/#python-docstrings","title":"Python Docstrings","text":"<pre><code>def conv2d(image, kernel):\n    \"\"\"\n    Apply 2D convolution to an image.\n\n    Args:\n        image: Input image array (H, W) or (H, W, C)\n        kernel: Convolution kernel (K, K)\n\n    Returns:\n        Convolved image array with same shape as input\n    \"\"\"\n    pass\n</code></pre>"},{"location":"pybind11/best-practices/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"pybind11/best-practices/#dont-over-optimize","title":"Don't Over-Optimize","text":"<ul> <li>Start with Python, optimize later</li> <li>Profile before optimizing</li> <li>Only optimize when it matters</li> </ul>"},{"location":"pybind11/best-practices/#dont-ignore-error-handling","title":"Don't Ignore Error Handling","text":"<ul> <li>Always validate inputs</li> <li>Handle edge cases</li> <li>Provide meaningful error messages</li> </ul>"},{"location":"pybind11/best-practices/#dont-skip-testing","title":"Don't Skip Testing","text":"<ul> <li>Test both implementations</li> <li>Test edge cases</li> <li>Test performance</li> </ul>"},{"location":"pybind11/best-practices/#pro-tips","title":"Pro Tips","text":"<ul> <li>Start simple - Get it working first, optimize later</li> <li>Use both implementations - Python for debugging, C++ for performance</li> <li>Profile everything - Measure before optimizing</li> <li>Document as you go - Don't leave it for later</li> <li>Test early and often - Catch issues before they become problems</li> </ul>"},{"location":"pybind11/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Simple Tutorial for hands-on practice</li> <li>Check Testing for robust testing strategies</li> <li>Explore Resources for advanced techniques</li> </ul>"},{"location":"pybind11/pybind11-basics/","title":"PyBind11 Basics","text":"<p>PyBind11 is a header-only library that exposes C++ code to Python. Think of it as a translator that handles type conversions, memory management, and NumPy integration.</p>"},{"location":"pybind11/pybind11-basics/#core-concepts","title":"Core Concepts","text":""},{"location":"pybind11/pybind11-basics/#module-definition","title":"Module Definition","text":"<pre><code>PYBIND11_MODULE(module_name, module) {\n    // Your bindings go here\n}\n</code></pre>"},{"location":"pybind11/pybind11-basics/#function-binding","title":"Function Binding","text":"<pre><code>// Simple function\nmodule.def(\"function_name\", &amp;cpp_function, \"Documentation\");\n\n// With arguments\nmodule.def(\"function_name\", &amp;cpp_function,\n           py::arg(\"arg1\"), py::arg(\"arg2\") = default_value,\n           \"Documentation\");\n</code></pre>"},{"location":"pybind11/pybind11-basics/#numpy-array-handling","title":"NumPy Array Handling","text":"<pre><code>py::array_t&lt;float&gt; my_function(py::array_t&lt;float&gt; input) {\n    py::buffer_info buf_info = input.request();\n    float* ptr = static_cast&lt;float*&gt;(buf_info.ptr);\n    // Process data...\n    return output;\n}\n</code></pre>"},{"location":"pybind11/pybind11-basics/#essential-types","title":"Essential Types","text":"C++ Type Python Type Notes <code>int</code> <code>int</code> Automatic conversion <code>float</code> <code>float</code> Automatic conversion <code>std::vector&lt;T&gt;</code> <code>list</code> Automatic conversion <code>py::array_t&lt;T&gt;</code> <code>numpy.ndarray</code> NumPy arrays"},{"location":"pybind11/pybind11-basics/#common-patterns","title":"Common Patterns","text":""},{"location":"pybind11/pybind11-basics/#input-validation","title":"Input Validation","text":"<pre><code>if (input.ndim() != 2) {\n    throw std::runtime_error(\"Input must be 2D\");\n}\n</code></pre>"},{"location":"pybind11/pybind11-basics/#memory-access","title":"Memory Access","text":"<pre><code>py::buffer_info buf = input.request();\nfloat* ptr = static_cast&lt;float*&gt;(buf.ptr);\n</code></pre>"},{"location":"pybind11/pybind11-basics/#return-arrays","title":"Return Arrays","text":"<pre><code>py::array_t&lt;float&gt; output({height, width});\npy::buffer_info out_buf = output.request();\nstd::memcpy(out_buf.ptr, result.data(), size * sizeof(float));\nreturn output;\n</code></pre>"},{"location":"pybind11/pybind11-basics/#pro-tips","title":"Pro Tips","text":"<ul> <li>Always validate inputs - Check dimensions, types, and contiguity</li> <li>Use C-contiguous arrays - Much faster for processing</li> <li>Handle exceptions properly - They become Python exceptions</li> <li>Document your functions - PyBind11 uses the docstrings</li> </ul>"},{"location":"pybind11/pybind11-basics/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Simple Tutorial for hands-on practice</li> <li>Check Best Practices for advanced techniques</li> <li>Explore Testing for robust code</li> </ul>"},{"location":"pybind11/resources/","title":"Resources &amp; Learning Materials","text":"<p>Essential resources for mastering Python-C++ bindings and computer vision development.</p>"},{"location":"pybind11/resources/#core-documentation","title":"Core Documentation","text":""},{"location":"pybind11/resources/#pybind11","title":"PyBind11","text":"<ul> <li>PyBind11 Documentation - Complete reference and tutorial</li> <li>PyBind11 GitHub - Source code and examples</li> <li>PyBind11 Examples - Official examples</li> </ul>"},{"location":"pybind11/resources/#numpy","title":"NumPy","text":"<ul> <li>NumPy C API - NumPy array handling</li> <li>NumPy Array Interface - Understanding NumPy arrays</li> </ul>"},{"location":"pybind11/resources/#cmake","title":"CMake","text":"<ul> <li>CMake Documentation - Build system reference</li> <li>Modern CMake - Modern CMake techniques</li> </ul>"},{"location":"pybind11/resources/#learning-resources","title":"Learning Resources","text":""},{"location":"pybind11/resources/#python-c-integration","title":"Python-C++ Integration","text":"<ul> <li>Python C Extensions - Understanding Python extensions</li> <li>Cython Documentation - Alternative to PyBind11</li> </ul>"},{"location":"pybind11/resources/#c-best-practices","title":"C++ Best Practices","text":"<ul> <li>C++ Core Guidelines - Official C++ best practices</li> <li>C++ Reference - Complete C++ reference</li> </ul>"},{"location":"pybind11/resources/#computer-vision","title":"Computer Vision","text":"<ul> <li>OpenCV Documentation - Computer vision library</li> <li>scikit-image - Python image processing</li> </ul>"},{"location":"pybind11/resources/#performance--optimization","title":"Performance &amp; Optimization","text":""},{"location":"pybind11/resources/#c-performance","title":"C++ Performance","text":"<ul> <li>Intel Intrinsics Guide - SIMD optimization</li> <li>C++ Performance Tips - Performance best practices</li> </ul>"},{"location":"pybind11/resources/#python-performance","title":"Python Performance","text":"<ul> <li>Python Performance Tips - Python optimization</li> <li>NumPy Performance - NumPy optimization</li> </ul>"},{"location":"pybind11/resources/#tools--libraries","title":"Tools &amp; Libraries","text":""},{"location":"pybind11/resources/#development-tools","title":"Development Tools","text":"<ul> <li>Visual Studio Code - Great Python/C++ IDE</li> <li>CLion - Professional C++ IDE</li> <li>CMake Tools - VS Code CMake extension</li> </ul>"},{"location":"pybind11/resources/#testing--debugging","title":"Testing &amp; Debugging","text":"<ul> <li>pytest - Python testing framework</li> <li>Google Test - C++ testing framework</li> <li>Valgrind - Memory debugging tool</li> </ul>"},{"location":"pybind11/resources/#build-tools","title":"Build Tools","text":"<ul> <li>Ninja - Fast build system</li> <li>Conan - C++ package manager</li> <li>vcpkg - C++ library manager</li> </ul>"},{"location":"pybind11/resources/#community--support","title":"Community &amp; Support","text":""},{"location":"pybind11/resources/#forums--communities","title":"Forums &amp; Communities","text":"<ul> <li>PyBind11 Discussions - PyBind11 community</li> <li>Stack Overflow - PyBind11 questions</li> <li>Reddit r/cpp - C++ community</li> </ul>"},{"location":"pybind11/resources/#books","title":"Books","text":"<ul> <li>\"Effective C++\" by Scott Meyers - C++ best practices</li> <li>\"Python Cookbook\" by David Beazley - Advanced Python techniques</li> <li>\"Computer Vision: Algorithms and Applications\" by Richard Szeliski - CV fundamentals</li> </ul>"},{"location":"pybind11/resources/#pro-tips","title":"Pro Tips","text":"<ul> <li>Start with the basics - Master PyBind11 fundamentals before advanced techniques</li> <li>Read the source code - PyBind11 examples are excellent learning material</li> <li>Join the community - Ask questions and share knowledge</li> <li>Practice regularly - Build small projects to reinforce learning</li> <li>Profile everything - Measure before optimizing</li> </ul>"},{"location":"pybind11/resources/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Simple Tutorial for hands-on practice</li> <li>Check Best Practices for advanced techniques</li> <li>Explore Testing for robust development</li> </ul>"},{"location":"pybind11/simple-tutorial/","title":"Simple PyBind11 Tutorial","text":"<p>Let's build something basic - a simple function that adds two numbers. This is the \"Hello World\" of PyBind11.</p>"},{"location":"pybind11/simple-tutorial/#what-were-building","title":"What We're Building","text":"<p>A C++ function that adds two integers, bound to Python. Nothing fancy, just the fundamentals.</p>"},{"location":"pybind11/simple-tutorial/#step-1-c-function","title":"Step 1: C++ Function","text":"<p>Create a simple header:</p> <pre><code>// nextcv/_cpp/src/core/add.hpp\n#pragma once\n\nint add(int a, int b);\n</code></pre> <p>Implement it:</p> <pre><code>// nextcv/_cpp/src/core/add.cpp\n#include \"add.hpp\"\n\nint add(int a, int b) {\n    return a + b;\n}\n</code></pre>"},{"location":"pybind11/simple-tutorial/#step-2-python-binding","title":"Step 2: Python Binding","text":"<p>Add to your bindings file:</p> <pre><code>// nextcv/_cpp/src/bindings/bindings.cpp\n#include &lt;pybind11/pybind11.h&gt;\n#include \"../core/add.hpp\"\n\nPYBIND11_MODULE(nextcv_py, module) {\n    module.def(\"add\", &amp;add, \"Add two integers\");\n}\n</code></pre>"},{"location":"pybind11/simple-tutorial/#step-3-use-it","title":"Step 3: Use It","text":"<pre><code>import nextcv as cvx\n\nresult = cvx.add(5, 3)  # Returns 8\nprint(result)\n</code></pre>"},{"location":"pybind11/simple-tutorial/#thats-it","title":"That's It!","text":"<p>You've just created your first PyBind11 binding. The C++ function is now available in Python with zero overhead.</p>"},{"location":"pybind11/simple-tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Try binding a function that takes NumPy arrays</li> <li>Add error handling</li> <li>Explore more complex data types</li> </ul> <p>Remember: Start simple, then add complexity as needed.</p>"},{"location":"pybind11/testing/","title":"Testing &amp; Debugging","text":"<p>Essential testing strategy for NextCV C++-Python bindings.</p>"},{"location":"pybind11/testing/#essential-tests","title":"Essential Tests","text":""},{"location":"pybind11/testing/#test-both-implementations","title":"Test Both Implementations","text":"<pre><code>import numpy as np\nimport nextcv.postprocessing as pp\n\ndef test_nms_implementations():\n    \"\"\"Test that C++ and Python NMS produce the same result.\"\"\"\n    bboxes = np.array([\n        [10, 10, 50, 50],   # Box 0\n        [15, 15, 55, 55],   # Box 1 (overlaps with 0)\n        [100, 100, 150, 150] # Box 2 (separate)\n    ], dtype=np.float32)\n\n    scores = np.array([0.9, 0.8, 0.7], dtype=np.float32)\n\n    # Compare implementations\n    result_cpp = pp.nms_cpp(bboxes, scores, 0.5)\n    result_py = pp.nms_np(bboxes, scores, 0.5)\n    np.testing.assert_array_equal(result_cpp, result_py)\n</code></pre>"},{"location":"pybind11/testing/#test-edge-cases","title":"Test Edge Cases","text":"<pre><code>def test_edge_cases():\n    \"\"\"Test empty and single inputs.\"\"\"\n    # Empty input\n    empty_bboxes = np.array([], dtype=np.float32).reshape(0, 4)\n    empty_scores = np.array([], dtype=np.float32)\n    result = pp.nms_np(empty_bboxes, empty_scores, 0.5)\n    assert len(result) == 0\n\n    # Single box\n    single_bbox = np.array([[10, 10, 50, 50]], dtype=np.float32)\n    single_score = np.array([0.9], dtype=np.float32)\n    result = pp.nms_np(single_bbox, single_score, 0.5)\n    assert len(result) == 1\n</code></pre>"},{"location":"pybind11/testing/#test-image-operations","title":"Test Image Operations","text":"<pre><code>import nextcv.image.ops as ops\n\ndef test_invert_function():\n    \"\"\"Test image inversion.\"\"\"\n    image = np.array([[[255, 0, 0], [0, 255, 0]]], dtype=np.uint8)\n    inverted = ops.invert(image)\n\n    assert inverted.shape == image.shape\n    assert inverted[0, 0, 0] == 0  # 255 -&gt; 0\n</code></pre>"},{"location":"pybind11/testing/#debugging-tips","title":"Debugging Tips","text":""},{"location":"pybind11/testing/#use-python-fallbacks","title":"Use Python Fallbacks","text":"<pre><code># Force Python implementation for debugging\npp.nms_cpp = None\nresult = pp.nms_np(bboxes, scores, 0.5)  # Uses Python only\n</code></pre>"},{"location":"pybind11/testing/#test-invalid-inputs","title":"Test Invalid Inputs","text":"<pre><code>import pytest\n\ndef test_invalid_inputs():\n    \"\"\"Test error handling.\"\"\"\n    bboxes = np.array([[10, 10, 50, 50]], dtype=np.float32)\n    scores = np.array([0.9], dtype=np.float32)\n\n    # Wrong number of scores\n    with pytest.raises(ValueError):\n        pp.nms_np(bboxes, scores[:0], 0.5)\n</code></pre>"},{"location":"pybind11/testing/#common-issues","title":"Common Issues","text":"<ul> <li>Memory Leaks - Use RAII in C++ and proper NumPy handling</li> <li>Type Mismatches - Validate <code>dtype</code> and <code>shape</code> before processing</li> <li>Performance Regression - Profile with <code>%timeit</code> in Jupyter</li> <li>Bounding Box Format - Always use (x1, y1, x2, y2) format</li> </ul>"},{"location":"pybind11/testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/\n\n# Run specific module\npytest tests/postprocessing/\n\n# Run specific test\npytest tests/postprocessing/test_boxes.py::test_nms_np\n</code></pre>"},{"location":"pybind11/testing/#pro-tips","title":"Pro Tips","text":"<ul> <li>Compare all implementations - C++, Python, and OpenCV</li> <li>Test edge cases - Empty boxes, single boxes, overlapping boxes</li> <li>Use Python fallbacks - Debug complex logic in Python first</li> <li>Profile performance - Ensure C++ actually helps</li> </ul>"},{"location":"pybind11/when-to-use-cpp/","title":"When to Use C++","text":"<p>Before you write a single line of C++, ask yourself one question: Do I really need it?</p> <p>C++ adds complexity. Use it only when it provides a meaningful impact.</p> <p>The Golden Rule</p> <p>Only use C++ for real performance bottlenecks that can't be solved with existing Python tools like NumPy or Numba.</p>"},{"location":"pybind11/when-to-use-cpp/#-the-decision-making-flowchart","title":"\ud83e\udde0 The Decision-Making Flowchart","text":"<p>Here's a simple flowchart to guide your decision:</p> <pre><code>graph LR\n    A[Start] --&gt; B{A good Python library exists?};\n    B -- \u2705 Yes --&gt; C[Use it!];\n    B -- \u274c No --&gt; D{Is it a real performance bottleneck?};\n    D -- \u274c No --&gt; E[Stick with Python];\n    D -- \u2705 Yes --&gt; F{Can it be optimized with NumPy/Numba?};\n    F -- \u2705 Yes --&gt; G[Use NumPy/Numba];\n    F -- \u274c No --&gt; H[\ud83d\ude80 Write it in C++];</code></pre>"},{"location":"pybind11/when-to-use-cpp/#-good-use-cases","title":"\u2705 Good Use Cases","text":"<p>When does C++ make sense? Here are a few examples:</p> Use Case Why C++? Example Non-Maximum Suppression Tight loops over thousands of boxes <code>nextcv.postprocessing.nms</code> Hungarian Algorithm Complex algorithm with O(n\u00b3) complexity <code>scipy.optimize.linear_sum_assignment</code> Low-Level Hardware Direct memory access for sensors/GPIO Interfacing with custom hardware"},{"location":"pybind11/when-to-use-cpp/#-common-pitfalls","title":"\ud83d\udea9 Common Pitfalls","text":"<p>Avoid these common traps:</p> Pitfall Description Better Approach \ud83c\udfc3\u200d\u2642\ufe0f \"C++ is always faster!\" Assuming C++ is a magic bullet. Profile first. Get concrete numbers. \ud83d\udd27 Reinventing the Wheel Writing C++ for a solved problem. Check SciPy, NumPy, and other libraries first."},{"location":"pybind11/when-to-use-cpp/#-final-checklist","title":"\u2705 Final Checklist","text":"<p>Use this checklist before you start writing C++:</p> <ul> <li> No good Python library exists for the task.</li> <li> Profiling shows a clear, measurable bottleneck.</li> <li> The bottleneck is a real issue in production.</li> <li> C++ will provide a significant (e.g., &gt;5x) speedup.</li> <li> The team is comfortable maintaining the C++ code.</li> </ul> <p>If you can't check all these boxes, stick with Python. \ud83d\udc0d</p>"},{"location":"reference/","title":"nextcv","text":""},{"location":"reference/#nextcv","title":"nextcv","text":"<p>NextCV: Python-first computer vision library with C++ bdingings for speed.</p> <p>This package provides both high-performance C++ wrapped functions and pure Python implementations in functional modules.</p> Usage <p>import nextcv as cvx</p> MODULE DESCRIPTION <code>core</code> <p>NextCV Core module - Core functionality.</p> <code>image</code> <p>NextCV Image module - Image processing functionality.</p> <code>linalg</code> <p>NextCV Linear Algebra module - Linear algebra functionality using Eigen.</p> <code>postprocessing</code> <p>NextCV Postprocessing module - Post-processing functionality.</p> <code>sensors</code> <p>NextCV Sensors module - Sensor representation and manipulation utilities.</p>"},{"location":"reference/#nextcv--c-wrapped-functions","title":"C++ wrapped functions","text":"<p>cvx.image.invert(image) cvx.postprocessing.nms_fast(boxes, 0.5)</p>"},{"location":"reference/#nextcv--python-implementations","title":"Python implementations","text":"<p>cvx.postprocessing.nms(boxes, 0.5) cvx.core.hello()</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nextcv</li> <li>core</li> <li>image<ul> <li>color</li> <li>compose</li> <li>geometry</li> <li>ops</li> <li>stitching</li> </ul> </li> <li>linalg</li> <li>postprocessing<ul> <li>boxes</li> </ul> </li> <li>sensors<ul> <li>camera</li> <li>parsers</li> </ul> </li> </ul>"},{"location":"reference/core/","title":"core","text":""},{"location":"reference/core/#nextcv.core","title":"nextcv.core","text":"<p>NextCV Core module - Core functionality.</p> FUNCTION DESCRIPTION <code>hello_cpp</code> <p>C++ implementation of hello function.</p> <code>hello_python</code> <p>Python implementation of hello function.</p>"},{"location":"reference/core/#nextcv.core.hello_cpp","title":"nextcv.core.hello_cpp","text":"<pre><code>hello_cpp() -&gt; str\n</code></pre> <p>C++ implementation of hello function.</p>"},{"location":"reference/core/#nextcv.core.hello_python","title":"nextcv.core.hello_python","text":"<pre><code>hello_python() -&gt; str\n</code></pre> <p>Python implementation of hello function.</p>"},{"location":"reference/image/","title":"image","text":""},{"location":"reference/image/#nextcv.image","title":"nextcv.image","text":"<p>NextCV Image module - Image processing functionality.</p> MODULE DESCRIPTION <code>color</code> <p>Color and intensity transformations.</p> <code>compose</code> <p>Image composition utilities.</p> <code>geometry</code> <p>Geometric image transformations.</p> <code>ops</code> <p>Image operations.</p> <code>stitching</code> <p>Generalized image stitching framework.</p> CLASS DESCRIPTION <code>LeftRightStitcher</code> <p>Stitch cameras arranged horizontally (left-to-right).</p> FUNCTION DESCRIPTION <code>invert</code> <p>Invert the image.</p>"},{"location":"reference/image/#nextcv.image.LeftRightStitcher","title":"nextcv.image.LeftRightStitcher","text":"<pre><code>LeftRightStitcher(left_camera: PinholeCamera, right_camera: PinholeCamera)\n</code></pre> <p>               Bases: <code>HorizontalStitcher</code></p> <p>Stitch cameras arranged horizontally (left-to-right).</p>"},{"location":"reference/image/#nextcv.image.invert","title":"nextcv.image.invert","text":"<pre><code>invert(image: NDArray[uint8]) -&gt; NDArray[uint8]\n</code></pre> <p>Invert the image.</p>"},{"location":"reference/image/color/","title":"color","text":""},{"location":"reference/image/color/#nextcv.image.color","title":"nextcv.image.color","text":"<p>Color and intensity transformations.</p> <p>Functions here adjust image brightness, contrast, and color representation. Includes operations like: - Colorspace conversion (e.g., BGR \u2194 GRAY, HSV, LAB) - Channel split/merge and per-channel adjustments - Histogram equalization and CLAHE for local contrast enhancement - Gamma correction and other tone-mapping techniques</p>"},{"location":"reference/image/compose/","title":"compose","text":""},{"location":"reference/image/compose/#nextcv.image.compose","title":"nextcv.image.compose","text":"<p>Image composition utilities.</p> <p>Functions here combine two or more images into a single result. Includes operations like: - Simple overlays and alpha blending - Masked compositing - Pairwise stitching (left/right) - Multi-image panorama creation</p> <p>These functions typically perform geometric alignment, warping, and optional seam blending to produce seamless composites.</p>"},{"location":"reference/image/geometry/","title":"geometry","text":""},{"location":"reference/image/geometry/#nextcv.image.geometry","title":"nextcv.image.geometry","text":"<p>Geometric image transformations.</p> <p>Functions here modify image shape, size, or pixel layout. Includes operations like: - Resizing with interpolation - Rotation, translation, and affine transforms - Perspective warping and remapping - Flipping, cropping, and padding</p>"},{"location":"reference/image/ops/","title":"ops","text":""},{"location":"reference/image/ops/#nextcv.image.ops","title":"nextcv.image.ops","text":"<p>Image operations.</p> <p>Functions here modify pixel values without changing image layout. Includes operations like: - Pixel-wise arithmetic and logic - Thresholding and binarization</p> FUNCTION DESCRIPTION <code>invert</code> <p>Invert the image.</p>"},{"location":"reference/image/ops/#nextcv.image.ops.invert","title":"nextcv.image.ops.invert","text":"<pre><code>invert(image: NDArray[uint8]) -&gt; NDArray[uint8]\n</code></pre> <p>Invert the image.</p>"},{"location":"reference/image/stitching/","title":"stitching","text":""},{"location":"reference/image/stitching/#nextcv.image.stitching","title":"nextcv.image.stitching","text":"<p>Generalized image stitching framework.</p> <p>Simple, extensible architecture for stitching images from multiple cameras.</p> CLASS DESCRIPTION <code>AdditiveCompensator</code> <p>Sequential additive bias correction.</p> <code>ExposureCompensator</code> <p>Abstract base class for exposure compensation strategies.</p> <code>HorizontalStitcher</code> <p>Stitch cameras arranged horizontally (left-to-right).</p> <code>ImageStitcher</code> <p>Abstract base class for image stitching.</p> <code>LeftRightStitcher</code> <p>Stitch cameras arranged horizontally (left-to-right).</p> <code>NoOpCompensator</code> <p>No compensation - returns images unchanged.</p> <code>Rect</code> <p>Image region defined by top-left corner and dimensions.</p> <code>Tile</code> <p>A tile within a canvas for image stitching.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.AdditiveCompensator","title":"nextcv.image.stitching.AdditiveCompensator","text":"<p>               Bases: <code>ExposureCompensator</code></p> <p>Sequential additive bias correction.</p> <p>Computes median intensity difference in overlap regions and applies additive corrections. First image is used as reference. Each subsequent image is corrected to match the already-corrected previous image.</p> <p>Good for small-to-medium brightness differences in thermal cameras.</p> METHOD DESCRIPTION <code>__call__</code> <p>Compute and apply additive bias corrections.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.AdditiveCompensator.__call__","title":"__call__","text":"<pre><code>__call__(tiles: List[Tile], images: List[ndarray]) -&gt; List[ndarray]\n</code></pre> <p>Compute and apply additive bias corrections.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.AdditiveCompensator._compute_bias_between_tiles","title":"_compute_bias_between_tiles  <code>staticmethod</code>","text":"<pre><code>_compute_bias_between_tiles(tile_i: Tile, tile_j: Tile, img_i: ndarray, img_j: ndarray) -&gt; float\n</code></pre> <p>Compute intensity bias between two overlapping tiles.</p> PARAMETER DESCRIPTION <code>tile_i</code> <p>First tile (reference)</p> <p> TYPE: <code>Tile</code> </p> <code>tile_j</code> <p>Second tile (to be corrected)</p> <p> TYPE: <code>Tile</code> </p> <code>img_i</code> <p>Warped image from tile_i</p> <p> TYPE: <code>ndarray</code> </p> <code>img_j</code> <p>Warped image from tile_j</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Bias to add to img_j to match img_i (float scalar)</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ExposureCompensator","title":"nextcv.image.stitching.ExposureCompensator","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for exposure compensation strategies.</p> <p>Compensators compute and apply corrections to warped images to minimize intensity differences in overlap regions.</p> METHOD DESCRIPTION <code>__call__</code> <p>Compute and apply corrections to warped images.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ExposureCompensator.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(tiles: List[Tile], warped_images: List[ndarray]) -&gt; List[ndarray]\n</code></pre> <p>Compute and apply corrections to warped images.</p> PARAMETER DESCRIPTION <code>tiles</code> <p>List of tiles with rect, mask, and weights</p> <p> TYPE: <code>List[Tile]</code> </p> <code>warped_images</code> <p>List of warped images (one per tile)</p> <p> TYPE: <code>List[ndarray]</code> </p> RETURNS DESCRIPTION <code>List[ndarray]</code> <p>List of corrected images</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.HorizontalStitcher","title":"nextcv.image.stitching.HorizontalStitcher","text":"<pre><code>HorizontalStitcher(cameras: List[CameraType], compensator: Optional[ExposureCompensator] = None)\n</code></pre> <p>               Bases: <code>ImageStitcher[PinholeCamera]</code></p> <p>Stitch cameras arranged horizontally (left-to-right).</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ImageStitcher","title":"nextcv.image.stitching.ImageStitcher","text":"<pre><code>ImageStitcher(cameras: List[CameraType], compensator: Optional[ExposureCompensator] = None)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[CameraType]</code></p> <p>Abstract base class for image stitching.</p> PARAMETER DESCRIPTION <code>cameras</code> <p>List of cameras to stitch</p> <p> TYPE: <code>List[CameraType]</code> </p> <code>compensator</code> <p>Exposure compensator to use. Defaults to NoOpCompensator()</p> <p> TYPE: <code>Optional[ExposureCompensator]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>__call__</code> <p>Stitch images into panorama.</p> <code>create_warp_tiles</code> <p>Create normalized warp tiles for all cameras.</p> <code>stitch</code> <p>Stitch images into the virtual camera.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ImageStitcher.__call__","title":"__call__","text":"<pre><code>__call__(images: List[ndarray]) -&gt; ndarray\n</code></pre> <p>Stitch images into panorama.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ImageStitcher._create_virtual_cam","title":"_create_virtual_cam  <code>abstractmethod</code>","text":"<pre><code>_create_virtual_cam() -&gt; CameraType\n</code></pre> <p>Create virtual camera defining output coordinate system.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ImageStitcher._sanity_checks","title":"_sanity_checks","text":"<pre><code>_sanity_checks(images: List[ndarray]) -&gt; None\n</code></pre> <p>Sanity checks for the images.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ImageStitcher.create_warp_tiles","title":"create_warp_tiles  <code>staticmethod</code>","text":"<pre><code>create_warp_tiles(cameras: List[CameraType], canvas: CameraType) -&gt; List[Tile]\n</code></pre> <p>Create normalized warp tiles for all cameras.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.ImageStitcher.stitch","title":"stitch","text":"<pre><code>stitch(images: List[ndarray]) -&gt; ndarray\n</code></pre> <p>Stitch images into the virtual camera.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.LeftRightStitcher","title":"nextcv.image.stitching.LeftRightStitcher","text":"<pre><code>LeftRightStitcher(left_camera: PinholeCamera, right_camera: PinholeCamera)\n</code></pre> <p>               Bases: <code>HorizontalStitcher</code></p> <p>Stitch cameras arranged horizontally (left-to-right).</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.NoOpCompensator","title":"nextcv.image.stitching.NoOpCompensator","text":"<p>               Bases: <code>ExposureCompensator</code></p> <p>No compensation - returns images unchanged.</p> <p>Fastest option. Use when images are already well-balanced or when exposure differences are negligible.</p> METHOD DESCRIPTION <code>__call__</code> <p>Return images unchanged.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.NoOpCompensator.__call__","title":"__call__","text":"<pre><code>__call__(tiles: List[Tile], warped_images: List[ndarray]) -&gt; List[ndarray]\n</code></pre> <p>Return images unchanged.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Rect","title":"nextcv.image.stitching.Rect  <code>dataclass</code>","text":"<pre><code>Rect(x: int, y: int, w: int, h: int)\n</code></pre> <p>Image region defined by top-left corner and dimensions.</p> PARAMETER DESCRIPTION <code>x</code> <p> TYPE: <code>int</code> </p> <code>y</code> <p> TYPE: <code>int</code> </p> <code>w</code> <p> TYPE: <code>int</code> </p> <code>h</code> <p> TYPE: <code>int</code> </p> METHOD DESCRIPTION <code>clamp_to</code> <p>Clamp region to bounds, return None if no overlap.</p> <code>intersect</code> <p>Compute intersection with another rectangle.</p> <code>numpy_slices</code> <p>Return (y_slice, x_slice) for numpy indexing with optional offset.</p> ATTRIBUTE DESCRIPTION <code>s</code> <p>Shorthand for numpy_slices() - returns (y_slice, x_slice).</p> <p> TYPE: <code>Tuple[slice, slice]</code> </p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Rect.s","title":"s  <code>property</code>","text":"<pre><code>s: Tuple[slice, slice]\n</code></pre> <p>Shorthand for numpy_slices() - returns (y_slice, x_slice).</p> Usage <p>arr[rect.s] = value</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Rect.clamp_to","title":"clamp_to","text":"<pre><code>clamp_to(max_w: int, max_h: int) -&gt; Optional[Rect]\n</code></pre> <p>Clamp region to bounds, return None if no overlap.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Rect.intersect","title":"intersect","text":"<pre><code>intersect(other: Rect) -&gt; Optional[Rect]\n</code></pre> <p>Compute intersection with another rectangle.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Rect.numpy_slices","title":"numpy_slices","text":"<pre><code>numpy_slices(offset_x: int = 0, offset_y: int = 0) -&gt; Tuple[slice, slice]\n</code></pre> <p>Return (y_slice, x_slice) for numpy indexing with optional offset.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Tile","title":"nextcv.image.stitching.Tile  <code>dataclass</code>","text":"<pre><code>Tile(rect: Rect, maps: Tuple[ndarray, ndarray], mask: ndarray, weights: ndarray)\n</code></pre> <p>A tile within a canvas for image stitching.</p> PARAMETER DESCRIPTION <code>rect</code> <p> TYPE: <code>Rect</code> </p> <code>maps</code> <p> TYPE: <code>Tuple[ndarray, ndarray]</code> </p> <code>mask</code> <p> TYPE: <code>ndarray</code> </p> <code>weights</code> <p> TYPE: <code>ndarray</code> </p> METHOD DESCRIPTION <code>from_camera_pair</code> <p>Create a warp tile from a camera.</p> <code>update_weights</code> <p>Update the weights.</p> <code>warp_image</code> <p>Warp source image to this tile.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Tile.from_camera_pair","title":"from_camera_pair  <code>classmethod</code>","text":"<pre><code>from_camera_pair(camera: CameraType, canvas: CameraType) -&gt; Optional[Tile]\n</code></pre> <p>Create a warp tile from a camera.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Tile.update_weights","title":"update_weights","text":"<pre><code>update_weights(weights: ndarray) -&gt; Tile\n</code></pre> <p>Update the weights.</p>"},{"location":"reference/image/stitching/#nextcv.image.stitching.Tile.warp_image","title":"warp_image","text":"<pre><code>warp_image(src_img: ndarray) -&gt; ndarray\n</code></pre> <p>Warp source image to this tile.</p>"},{"location":"reference/linalg/","title":"linalg","text":""},{"location":"reference/linalg/#nextcv.linalg","title":"nextcv.linalg","text":"<p>NextCV Linear Algebra module - Linear algebra functionality using Eigen.</p> FUNCTION DESCRIPTION <code>matvec</code> <p>Multiply matrix (MxN) by vector (N) \u2192 y (M). Uses Eigen.</p>"},{"location":"reference/linalg/#nextcv.linalg.matvec","title":"nextcv.linalg.matvec","text":"<pre><code>matvec(matrix: NDArray, vector: NDArray) -&gt; NDArray\n</code></pre> <p>Multiply matrix (MxN) by vector (N) \u2192 y (M). Uses Eigen.</p> PARAMETER DESCRIPTION <code>matrix</code> <p>Input matrix of shape (M, N)</p> <p> TYPE: <code>NDArray</code> </p> <code>vector</code> <p>Input vector of shape (N,)</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Result vector of shape (M,)</p>"},{"location":"reference/postprocessing/","title":"postprocessing","text":""},{"location":"reference/postprocessing/#nextcv.postprocessing","title":"nextcv.postprocessing","text":"<p>NextCV Postprocessing module - Post-processing functionality.</p> MODULE DESCRIPTION <code>boxes</code> <p>Bounding box postprocessing functions.</p> FUNCTION DESCRIPTION <code>nms_cpp</code> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p> <code>nms_np</code> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p>"},{"location":"reference/postprocessing/#nextcv.postprocessing.nms_cpp","title":"nextcv.postprocessing.nms_cpp","text":"<pre><code>nms_cpp(bboxes: NDArray, scores: NDArray, iou_thresh: float) -&gt; NDArray[int32]\n</code></pre> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p> PARAMETER DESCRIPTION <code>bboxes</code> <p>The bounding boxes as (top_left_x, top_left_y, bottom_right_x, bottom_right_y).</p> <p> TYPE: <code>NDArray</code> </p> <code>scores</code> <p>The confidence scores for each bounding box.</p> <p> TYPE: <code>NDArray</code> </p> <code>iou_thresh</code> <p>The threshold for intersection over union.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/postprocessing/#nextcv.postprocessing.nms_np","title":"nextcv.postprocessing.nms_np","text":"<pre><code>nms_np(bboxes: NDArray, scores: NDArray, iou_thresh: float) -&gt; NDArray\n</code></pre> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p> PARAMETER DESCRIPTION <code>bboxes</code> <p>The bounding boxes as (top_left_x, top_left_y, bottom_right_x, bottom_right_y).</p> <p> TYPE: <code>NDArray</code> </p> <code>scores</code> <p>The confidence scores for each bounding box.</p> <p> TYPE: <code>NDArray</code> </p> <code>iou_thresh</code> <p>The threshold for intersection over union.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>The indices of the bounding boxes to keep.</p>"},{"location":"reference/postprocessing/boxes/","title":"boxes","text":""},{"location":"reference/postprocessing/boxes/#nextcv.postprocessing.boxes","title":"nextcv.postprocessing.boxes","text":"<p>Bounding box postprocessing functions.</p> FUNCTION DESCRIPTION <code>iou_np</code> <p>Calculate intersection over union of target box with all others.</p> <code>nms_cpp</code> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p> <code>nms_np</code> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p>"},{"location":"reference/postprocessing/boxes/#nextcv.postprocessing.boxes.iou_np","title":"nextcv.postprocessing.boxes.iou_np","text":"<pre><code>iou_np(target_box: NDArray, boxes: NDArray, target_area: NDArray, areas: NDArray, inclusive: bool = False) -&gt; NDArray\n</code></pre> <p>Calculate intersection over union of target box with all others.</p> PARAMETER DESCRIPTION <code>target_box</code> <p>The bounding box as (top_left_x, top_left_y, bottom_right_x, bottom_right_y).</p> <p> TYPE: <code>NDArray</code> </p> <code>boxes</code> <p>The bounding boxes as (top_left_x, top_left_y, bottom_right_x, bottom_right_y).</p> <p> TYPE: <code>NDArray</code> </p> <code>target_area</code> <p>The area of the target box.</p> <p> TYPE: <code>NDArray</code> </p> <code>areas</code> <p>The areas of all boxes.</p> <p> TYPE: <code>NDArray</code> </p> <code>inclusive</code> <p>If True, uses (x2 - x1 + 1) * (y2 - y1 + 1) for area. Enable only for integer, pixel-indexed boxes (VOC/OpenCV style).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>The intersection over union of the target box with all others.</p>"},{"location":"reference/postprocessing/boxes/#nextcv.postprocessing.boxes.nms_cpp","title":"nextcv.postprocessing.boxes.nms_cpp","text":"<pre><code>nms_cpp(bboxes: NDArray, scores: NDArray, iou_thresh: float) -&gt; NDArray[int32]\n</code></pre> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p> PARAMETER DESCRIPTION <code>bboxes</code> <p>The bounding boxes as (top_left_x, top_left_y, bottom_right_x, bottom_right_y).</p> <p> TYPE: <code>NDArray</code> </p> <code>scores</code> <p>The confidence scores for each bounding box.</p> <p> TYPE: <code>NDArray</code> </p> <code>iou_thresh</code> <p>The threshold for intersection over union.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/postprocessing/boxes/#nextcv.postprocessing.boxes.nms_np","title":"nextcv.postprocessing.boxes.nms_np","text":"<pre><code>nms_np(bboxes: NDArray, scores: NDArray, iou_thresh: float) -&gt; NDArray\n</code></pre> <p>Non-Maximum-Supression (NMS) algorithm to remove overlapping bounding boxes.</p> PARAMETER DESCRIPTION <code>bboxes</code> <p>The bounding boxes as (top_left_x, top_left_y, bottom_right_x, bottom_right_y).</p> <p> TYPE: <code>NDArray</code> </p> <code>scores</code> <p>The confidence scores for each bounding box.</p> <p> TYPE: <code>NDArray</code> </p> <code>iou_thresh</code> <p>The threshold for intersection over union.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>The indices of the bounding boxes to keep.</p>"},{"location":"reference/sensors/","title":"sensors","text":""},{"location":"reference/sensors/#nextcv.sensors","title":"nextcv.sensors","text":"<p>NextCV Sensors module - Sensor representation and manipulation utilities.</p> MODULE DESCRIPTION <code>camera</code> <p>Sensor representation and manipulation utilities.</p> <code>parsers</code> <p>Parsers for sensor calibration data.</p> CLASS DESCRIPTION <code>CalibrationData</code> <p>Simple calibration data parser without heavy dependencies.</p> <code>Camera</code> <p>Represents a camera with its intrinsics, pose, and image resolution.</p> <code>PinholeCamera</code> <p>Represents a pinhole camera with its intrinsics and pose.</p>"},{"location":"reference/sensors/#nextcv.sensors.CalibrationData","title":"nextcv.sensors.CalibrationData  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Simple calibration data parser without heavy dependencies.</p> PARAMETER DESCRIPTION <code>cameras</code> <p>Dictionary of cameras with their calibration data</p> <p> TYPE: <code>Dict[str, PinholeCamera]</code> DEFAULT: <code>None</code> </p> <p>Fields:</p> <ul> <li> <code>cameras</code>                 (<code>Dict[str, PinholeCamera]</code>)             </li> </ul>"},{"location":"reference/sensors/#nextcv.sensors.CalibrationData.cameras","title":"cameras  <code>pydantic-field</code>","text":"<pre><code>cameras: Dict[str, PinholeCamera]\n</code></pre> <p>Dictionary of cameras with their calibration data</p>"},{"location":"reference/sensors/#nextcv.sensors.CalibrationData.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(file_path: Union[str, Path]) -&gt; CalibrationData\n</code></pre> <p>Parse calibration data from JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the JSON calibration file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>CalibrationData</code> <p>Parsed calibration data</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera","title":"nextcv.sensors.Camera  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a camera with its intrinsics, pose, and image resolution.</p> PARAMETER DESCRIPTION <code>width</code> <p>Image width in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>height</code> <p>Image height in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>fx</code> <p>Focal length along x-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>fy</code> <p>Focal length along y-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cx</code> <p>Principal point x-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cy</code> <p>Principal point y-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>roll</code> <p>Camera roll angle in degrees (rotation around z-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>pitch</code> <p>Camera pitch angle in degrees (rotation around x-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>yaw</code> <p>Camera yaw angle in degrees (rotation around y-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <p>Fields:</p> <ul> <li> <code>width</code>                 (<code>int</code>)             </li> <li> <code>height</code>                 (<code>int</code>)             </li> <li> <code>fx</code>                 (<code>float</code>)             </li> <li> <code>fy</code>                 (<code>float</code>)             </li> <li> <code>cx</code>                 (<code>float</code>)             </li> <li> <code>cy</code>                 (<code>float</code>)             </li> <li> <code>roll</code>                 (<code>float</code>)             </li> <li> <code>pitch</code>                 (<code>float</code>)             </li> <li> <code>yaw</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/sensors/#nextcv.sensors.Camera.cx","title":"cx  <code>pydantic-field</code>","text":"<pre><code>cx: float\n</code></pre> <p>Principal point x-coordinate in pixels</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.cy","title":"cy  <code>pydantic-field</code>","text":"<pre><code>cy: float\n</code></pre> <p>Principal point y-coordinate in pixels</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.fx","title":"fx  <code>pydantic-field</code>","text":"<pre><code>fx: float\n</code></pre> <p>Focal length along x-axis in pixels</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.fy","title":"fy  <code>pydantic-field</code>","text":"<pre><code>fy: float\n</code></pre> <p>Focal length along y-axis in pixels</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.height","title":"height  <code>pydantic-field</code>","text":"<pre><code>height: int\n</code></pre> <p>Image height in pixels</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.pitch","title":"pitch  <code>pydantic-field</code>","text":"<pre><code>pitch: float\n</code></pre> <p>Camera pitch angle in degrees (rotation around x-axis)</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.roll","title":"roll  <code>pydantic-field</code>","text":"<pre><code>roll: float\n</code></pre> <p>Camera roll angle in degrees (rotation around z-axis)</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.size","title":"size  <code>property</code>","text":"<pre><code>size: Tuple[int, int]\n</code></pre> <p>Get the camera size as an opencv-compatible tuple of width and height.</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.width","title":"width  <code>pydantic-field</code>","text":"<pre><code>width: int\n</code></pre> <p>Image width in pixels</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.yaw","title":"yaw  <code>pydantic-field</code>","text":"<pre><code>yaw: float\n</code></pre> <p>Camera yaw angle in degrees (rotation around y-axis)</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.compute_homography_to","title":"compute_homography_to","text":"<pre><code>compute_homography_to(target: Camera, neg_focal_length: bool = True) -&gt; ndarray\n</code></pre> <p>Compute homography matrix that maps points from this cam to target cam.</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any]) -&gt; T\n</code></pre> <p>Create a Camera instance from intrinsics and pose dictionaries.</p> PARAMETER DESCRIPTION <code>data</code> <p>Dictionary with keys \"fx\", \"fy\", \"cx\", \"cy\", \"width\", \"height\", \"roll\", \"pitch\", \"yaw\"</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>T</code> <p>Camera instance of the correct subclass type</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.maps_from","title":"maps_from","text":"<pre><code>maps_from(src: Camera, neg_focal_length: bool = True) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Create remapping maps from source camera to this camera.</p>"},{"location":"reference/sensors/#nextcv.sensors.Camera.must_be_even","title":"must_be_even","text":"<pre><code>must_be_even(v: int) -&gt; int\n</code></pre> <p>Ensure width and height are positive integers.</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera","title":"nextcv.sensors.PinholeCamera  <code>pydantic-model</code>","text":"<p>               Bases: <code>Camera</code></p> <p>Represents a pinhole camera with its intrinsics and pose.</p> PARAMETER DESCRIPTION <code>width</code> <p>Image width in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>height</code> <p>Image height in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>fx</code> <p>Focal length along x-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>fy</code> <p>Focal length along y-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cx</code> <p>Principal point x-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cy</code> <p>Principal point y-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>roll</code> <p>Camera roll angle in degrees (rotation around z-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>pitch</code> <p>Camera pitch angle in degrees (rotation around x-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>yaw</code> <p>Camera yaw angle in degrees (rotation around y-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <p>Fields:</p> <ul> <li> <code>width</code>                 (<code>int</code>)             </li> <li> <code>height</code>                 (<code>int</code>)             </li> <li> <code>fx</code>                 (<code>float</code>)             </li> <li> <code>fy</code>                 (<code>float</code>)             </li> <li> <code>cx</code>                 (<code>float</code>)             </li> <li> <code>cy</code>                 (<code>float</code>)             </li> <li> <code>roll</code>                 (<code>float</code>)             </li> <li> <code>pitch</code>                 (<code>float</code>)             </li> <li> <code>yaw</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.K","title":"K  <code>property</code>","text":"<pre><code>K: ndarray\n</code></pre> <p>Get the camera intrinsics matrix.</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.R","title":"R  <code>property</code>","text":"<pre><code>R: ndarray\n</code></pre> <p>Get the camera rotation matrix from Euler angles with pinhole convention.</p> <p>The pinhole convention is: - x is right - y is down - z is forward</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.compute_homography_to","title":"compute_homography_to","text":"<pre><code>compute_homography_to(target: PinholeCamera, neg_focal_length: bool = True) -&gt; ndarray\n</code></pre> <p>Compute homography matrix that maps points from this camera to target camera.</p> PARAMETER DESCRIPTION <code>target</code> <p>Target camera to compute homography to</p> <p> TYPE: <code>PinholeCamera</code> </p> <code>neg_focal_length</code> <p>Whether to negate the focal length</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Homography matrix that transforms points from this camera to target camera</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.crop","title":"crop","text":"<pre><code>crop(left: float, top: float, right: float, bottom: float, force_even: bool = True) -&gt; PinholeCamera\n</code></pre> <p>Return a new PinholeCamera cropped by margins (left, top, right, bottom).</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.hconcat","title":"hconcat  <code>classmethod</code>","text":"<pre><code>hconcat(left: PinholeCamera, right: PinholeCamera) -&gt; PinholeCamera\n</code></pre> <p>Create a virtual camera by concatenating left and right cams horizontally.</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.maps_from","title":"maps_from","text":"<pre><code>maps_from(src: PinholeCamera, neg_focal_length: bool = True) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Create remapping maps from source camera to this camera.</p>"},{"location":"reference/sensors/#nextcv.sensors.PinholeCamera.vconcat","title":"vconcat  <code>classmethod</code>","text":"<pre><code>vconcat(top: PinholeCamera, bottom: PinholeCamera) -&gt; PinholeCamera\n</code></pre> <p>Create a virtual camera by concatenating top and bottom cams vertically.</p>"},{"location":"reference/sensors/camera/","title":"camera","text":""},{"location":"reference/sensors/camera/#nextcv.sensors.camera","title":"nextcv.sensors.camera","text":"<p>Sensor representation and manipulation utilities.</p> CLASS DESCRIPTION <code>Camera</code> <p>Represents a camera with its intrinsics, pose, and image resolution.</p> <code>PinholeCamera</code> <p>Represents a pinhole camera with its intrinsics and pose.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera","title":"nextcv.sensors.camera.Camera  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a camera with its intrinsics, pose, and image resolution.</p> PARAMETER DESCRIPTION <code>width</code> <p>Image width in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>height</code> <p>Image height in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>fx</code> <p>Focal length along x-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>fy</code> <p>Focal length along y-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cx</code> <p>Principal point x-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cy</code> <p>Principal point y-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>roll</code> <p>Camera roll angle in degrees (rotation around z-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>pitch</code> <p>Camera pitch angle in degrees (rotation around x-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>yaw</code> <p>Camera yaw angle in degrees (rotation around y-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <p>Fields:</p> <ul> <li> <code>width</code>                 (<code>int</code>)             </li> <li> <code>height</code>                 (<code>int</code>)             </li> <li> <code>fx</code>                 (<code>float</code>)             </li> <li> <code>fy</code>                 (<code>float</code>)             </li> <li> <code>cx</code>                 (<code>float</code>)             </li> <li> <code>cy</code>                 (<code>float</code>)             </li> <li> <code>roll</code>                 (<code>float</code>)             </li> <li> <code>pitch</code>                 (<code>float</code>)             </li> <li> <code>yaw</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.cx","title":"cx  <code>pydantic-field</code>","text":"<pre><code>cx: float\n</code></pre> <p>Principal point x-coordinate in pixels</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.cy","title":"cy  <code>pydantic-field</code>","text":"<pre><code>cy: float\n</code></pre> <p>Principal point y-coordinate in pixels</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.fx","title":"fx  <code>pydantic-field</code>","text":"<pre><code>fx: float\n</code></pre> <p>Focal length along x-axis in pixels</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.fy","title":"fy  <code>pydantic-field</code>","text":"<pre><code>fy: float\n</code></pre> <p>Focal length along y-axis in pixels</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.height","title":"height  <code>pydantic-field</code>","text":"<pre><code>height: int\n</code></pre> <p>Image height in pixels</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.pitch","title":"pitch  <code>pydantic-field</code>","text":"<pre><code>pitch: float\n</code></pre> <p>Camera pitch angle in degrees (rotation around x-axis)</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.roll","title":"roll  <code>pydantic-field</code>","text":"<pre><code>roll: float\n</code></pre> <p>Camera roll angle in degrees (rotation around z-axis)</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.size","title":"size  <code>property</code>","text":"<pre><code>size: Tuple[int, int]\n</code></pre> <p>Get the camera size as an opencv-compatible tuple of width and height.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.width","title":"width  <code>pydantic-field</code>","text":"<pre><code>width: int\n</code></pre> <p>Image width in pixels</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.yaw","title":"yaw  <code>pydantic-field</code>","text":"<pre><code>yaw: float\n</code></pre> <p>Camera yaw angle in degrees (rotation around y-axis)</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.compute_homography_to","title":"compute_homography_to","text":"<pre><code>compute_homography_to(target: Camera, neg_focal_length: bool = True) -&gt; ndarray\n</code></pre> <p>Compute homography matrix that maps points from this cam to target cam.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any]) -&gt; T\n</code></pre> <p>Create a Camera instance from intrinsics and pose dictionaries.</p> PARAMETER DESCRIPTION <code>data</code> <p>Dictionary with keys \"fx\", \"fy\", \"cx\", \"cy\", \"width\", \"height\", \"roll\", \"pitch\", \"yaw\"</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>T</code> <p>Camera instance of the correct subclass type</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.maps_from","title":"maps_from","text":"<pre><code>maps_from(src: Camera, neg_focal_length: bool = True) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Create remapping maps from source camera to this camera.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.Camera.must_be_even","title":"must_be_even","text":"<pre><code>must_be_even(v: int) -&gt; int\n</code></pre> <p>Ensure width and height are positive integers.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera","title":"nextcv.sensors.camera.PinholeCamera  <code>pydantic-model</code>","text":"<p>               Bases: <code>Camera</code></p> <p>Represents a pinhole camera with its intrinsics and pose.</p> PARAMETER DESCRIPTION <code>width</code> <p>Image width in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>height</code> <p>Image height in pixels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>fx</code> <p>Focal length along x-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>fy</code> <p>Focal length along y-axis in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cx</code> <p>Principal point x-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>cy</code> <p>Principal point y-coordinate in pixels</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>roll</code> <p>Camera roll angle in degrees (rotation around z-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>pitch</code> <p>Camera pitch angle in degrees (rotation around x-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>yaw</code> <p>Camera yaw angle in degrees (rotation around y-axis)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <p>Fields:</p> <ul> <li> <code>width</code>                 (<code>int</code>)             </li> <li> <code>height</code>                 (<code>int</code>)             </li> <li> <code>fx</code>                 (<code>float</code>)             </li> <li> <code>fy</code>                 (<code>float</code>)             </li> <li> <code>cx</code>                 (<code>float</code>)             </li> <li> <code>cy</code>                 (<code>float</code>)             </li> <li> <code>roll</code>                 (<code>float</code>)             </li> <li> <code>pitch</code>                 (<code>float</code>)             </li> <li> <code>yaw</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.K","title":"K  <code>property</code>","text":"<pre><code>K: ndarray\n</code></pre> <p>Get the camera intrinsics matrix.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.R","title":"R  <code>property</code>","text":"<pre><code>R: ndarray\n</code></pre> <p>Get the camera rotation matrix from Euler angles with pinhole convention.</p> <p>The pinhole convention is: - x is right - y is down - z is forward</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.compute_homography_to","title":"compute_homography_to","text":"<pre><code>compute_homography_to(target: PinholeCamera, neg_focal_length: bool = True) -&gt; ndarray\n</code></pre> <p>Compute homography matrix that maps points from this camera to target camera.</p> PARAMETER DESCRIPTION <code>target</code> <p>Target camera to compute homography to</p> <p> TYPE: <code>PinholeCamera</code> </p> <code>neg_focal_length</code> <p>Whether to negate the focal length</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Homography matrix that transforms points from this camera to target camera</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.crop","title":"crop","text":"<pre><code>crop(left: float, top: float, right: float, bottom: float, force_even: bool = True) -&gt; PinholeCamera\n</code></pre> <p>Return a new PinholeCamera cropped by margins (left, top, right, bottom).</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.hconcat","title":"hconcat  <code>classmethod</code>","text":"<pre><code>hconcat(left: PinholeCamera, right: PinholeCamera) -&gt; PinholeCamera\n</code></pre> <p>Create a virtual camera by concatenating left and right cams horizontally.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.maps_from","title":"maps_from","text":"<pre><code>maps_from(src: PinholeCamera, neg_focal_length: bool = True) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Create remapping maps from source camera to this camera.</p>"},{"location":"reference/sensors/camera/#nextcv.sensors.camera.PinholeCamera.vconcat","title":"vconcat  <code>classmethod</code>","text":"<pre><code>vconcat(top: PinholeCamera, bottom: PinholeCamera) -&gt; PinholeCamera\n</code></pre> <p>Create a virtual camera by concatenating top and bottom cams vertically.</p>"},{"location":"reference/sensors/parsers/","title":"parsers","text":""},{"location":"reference/sensors/parsers/#nextcv.sensors.parsers","title":"nextcv.sensors.parsers","text":"<p>Parsers for sensor calibration data.</p> CLASS DESCRIPTION <code>CalibrationData</code> <p>Simple calibration data parser without heavy dependencies.</p>"},{"location":"reference/sensors/parsers/#nextcv.sensors.parsers.CalibrationData","title":"nextcv.sensors.parsers.CalibrationData  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Simple calibration data parser without heavy dependencies.</p> PARAMETER DESCRIPTION <code>cameras</code> <p>Dictionary of cameras with their calibration data</p> <p> TYPE: <code>Dict[str, PinholeCamera]</code> DEFAULT: <code>None</code> </p> <p>Fields:</p> <ul> <li> <code>cameras</code>                 (<code>Dict[str, PinholeCamera]</code>)             </li> </ul>"},{"location":"reference/sensors/parsers/#nextcv.sensors.parsers.CalibrationData.cameras","title":"cameras  <code>pydantic-field</code>","text":"<pre><code>cameras: Dict[str, PinholeCamera]\n</code></pre> <p>Dictionary of cameras with their calibration data</p>"},{"location":"reference/sensors/parsers/#nextcv.sensors.parsers.CalibrationData.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(file_path: Union[str, Path]) -&gt; CalibrationData\n</code></pre> <p>Parse calibration data from JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the JSON calibration file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>CalibrationData</code> <p>Parsed calibration data</p>"}]}